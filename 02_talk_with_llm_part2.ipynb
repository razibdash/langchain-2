{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74ca373",
   "metadata": {},
   "source": [
    "chat completion model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5687617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Langchain 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a41669ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "from langchain_groq import ChatGroq\n",
    "GROQ_API_KEY=os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c216e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Groq LLM with the model name and parameters\n",
    "llamaChatModel = ChatGroq(\n",
    "    api_key=GROQ_API_KEY,\n",
    "    model=\"llama3-70b-8192\",\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1aeaa2",
   "metadata": {},
   "source": [
    "Prompts and Prompt Templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2393d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph P. Kennedy, the patriarch of the Kennedy family, had a total of 29 grandchildren. Yes, you heard that right - 29! His children, including John F. Kennedy, Robert F. Kennedy, Ted Kennedy, and the others, had a large brood of kids, and many of them have continued to make their mark on American politics, business, and society.\n"
     ]
    }
   ],
   "source": [
    "# chat completion model\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an {profession} expert on {topic}.\"),\n",
    "        (\"human\", \"Hello, Mr. {profession}, can you please answer a question?\"),\n",
    "        (\"ai\", \"Sure!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    profession=\"Historian\",\n",
    "    topic=\"The Kennedy family\",\n",
    "    user_input=\"How many grandchildren had Joseph P. Kennedy?\"\n",
    ")\n",
    "\n",
    "response = llamaChatModel.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10930a",
   "metadata": {},
   "source": [
    "Old way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f79c9154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are an Historian expert on the Kennedy family.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    user_input=\"Name the children and grandchildren of Joseph P. Kennedy?\"\n",
    ")\n",
    "\n",
    "response = llamaChatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "174b81f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an historian expert on the Kennedy family, I'd be delighted to provide you with the list of children and grandchildren of Joseph P. Kennedy Sr.:\n",
      "\n",
      "**Children of Joseph P. Kennedy Sr. and Rose Fitzgerald Kennedy:**\n",
      "\n",
      "1. Joseph Patrick Kennedy Jr. (1915-1944)\n",
      "2. John Fitzgerald Kennedy (1917-1963) - 35th President of the United States\n",
      "3. Rosemary Kennedy (1918-2005)\n",
      "4. Kathleen Agnes Kennedy (1920-1948)\n",
      "5. Eunice Mary Kennedy (1921-2009)\n",
      "6. Patricia Helen Kennedy (1924-2006)\n",
      "7. Robert Francis Kennedy (1925-1968) - U.S. Attorney General and U.S. Senator\n",
      "8. Jean Ann Kennedy (b. 1928)\n",
      "9. Edward Moore Kennedy (1932-2009) - U.S. Senator\n",
      "\n",
      "**Grandchildren of Joseph P. Kennedy Sr.:**\n",
      "\n",
      "**Children of Joseph P. Kennedy Jr. (no children, as he died in 1944)**\n",
      "\n",
      "**Children of John F. Kennedy:**\n",
      "\n",
      "1. Arabella Kennedy (1956-1956) - stillborn\n",
      "2. Caroline Bouvier Kennedy (b. 1957)\n",
      "3. John Fitzgerald Kennedy Jr. (1960-1999)\n",
      "\n",
      "**Children of Rosemary Kennedy (no children)**\n",
      "\n",
      "**Children of Kathleen Kennedy:**\n",
      "\n",
      "1. Christopher George Cavendish (b. 1935) - son with William Cavendish, Marquess of Hartington\n",
      "2. Patricia Cavendish (b. 1937) - daughter with William Cavendish, Marquess of Hartington\n",
      "\n",
      "**Children of Eunice Kennedy:**\n",
      "\n",
      "1. Maria Shriver (b. 1955) - daughter with Sargent Shriver\n",
      "2. Timothy Perry Shriver (b. 1959) - son with Sargent Shriver\n",
      "3. Mark Kennedy Shriver (b. 1964) - son with Sargent Shriver\n",
      "4. Anthony Paul Kennedy Shriver (b. 1965) - son with Sargent Shriver\n",
      "\n",
      "**Children of Patricia Kennedy:**\n",
      "\n",
      "1. Christopher George Kennedy (b. 1958) - son with Peter Lawford\n",
      "2. Sydney Maleia Kennedy (b. 1959) - daughter with Peter Lawford\n",
      "3. Victoria Francis Lawford (b. 1961) - daughter with Peter Lawford\n",
      "4. Robin Elizabeth Lawford (b. 1963) - daughter with Peter Lawford\n",
      "\n",
      "**Children of Robert F. Kennedy:**\n",
      "\n",
      "1. Kathleen Kennedy Townsend (b. 1951) - daughter with Ethel Skakel Kennedy\n",
      "2. Joseph Patrick Kennedy II (b. 1952) - son with Ethel Skakel Kennedy\n",
      "3. Robert Francis Kennedy Jr. (b. 1954) - son with Ethel Skakel Kennedy\n",
      "4. David Anthony Kennedy (1955-1984) - son with Ethel Skakel Kennedy\n",
      "5. Mary Richardson Kennedy (1959-2012) - daughter with Ethel Skakel Kennedy\n",
      "6. Michael LeMoyne Kennedy (1960-1997) - son with Ethel Skakel Kennedy\n",
      "7. Mary Courtney Kennedy (b. 1962) - daughter with Ethel Skakel Kennedy\n",
      "8. Christopher George Kennedy (b. 1963) - son with Ethel Skakel Kennedy\n",
      "9. Matthew Maxwell Taylor Kennedy (b. 1965) - son with Ethel Skakel Kennedy\n",
      "10. Rory Kennedy (b. 1968) - daughter with Ethel Skakel Kennedy\n",
      "\n",
      "**Children of Jean Kennedy:**\n",
      "\n",
      "1. William Kennedy Smith (b. 1960) - son with Stephen Edward Smith\n",
      "2. Amanda Mary Smith (b. 1967) - daughter with Stephen Edward Smith\n",
      "3. Kym Maria Smith (b. 1972) - daughter with Stephen Edward Smith\n",
      "\n",
      "**Children of Edward M. Kennedy:**\n",
      "\n",
      "1. Kara Anne Kennedy (1960-2011) - daughter with Joan Bennett Kennedy\n",
      "2. Edward Moore Kennedy Jr. (b. 1961) - son with Joan Bennett Kennedy\n",
      "3. Patrick Joseph Kennedy II (b. 1967) - son with Joan Bennett Kennedy\n",
      "\n",
      "I hope this extensive list helps!\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde19d7",
   "metadata": {},
   "source": [
    "Few Shot Prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90e43a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿hola?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "response= llamaChatModel.invoke(final_prompt.format_messages(input=\"hi?\"))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7ef9a",
   "metadata": {},
   "source": [
    "Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b055582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡hola, cómo estás!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | llamaChatModel\n",
    "response = chain.invoke({\"input\": \"hi how are you?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3601d12",
   "metadata": {},
   "source": [
    "Output Parsers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78aedeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:918: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `ResponseSchema` to V2.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
    ")\n",
    "\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "json_chain = json_prompt | llamaChatModel | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92df5965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dc1bdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Russia'}\n"
     ]
    }
   ],
   "source": [
    "res  = json_chain.invoke({\"question\": \"What is the biggest country?\"})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1f87187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\langchain_groq\\chat_models.py:370: UserWarning: WARNING! max_completion_tokens is not default parameter.\n",
      "                    max_completion_tokens was transferred to model_kwargs.\n",
      "                    Please confirm that max_completion_tokens is what you intended.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\langchain_groq\\chat_models.py:370: UserWarning: WARNING! top_p is not default parameter.\n",
      "                    top_p was transferred to model_kwargs.\n",
      "                    Please confirm that top_p is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llmmodel=ChatGroq(\n",
    "    api_key=GROQ_API_KEY,\n",
    "    model=\"llama3-70b-8192\",\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"create a jokes!\", \"output\": \"Why don't programmers like nature?Because it has too many bugs. \"},\n",
    "    {\"input\": \"create a joke for teachers\", \"output\": \"Why did the teacher wear sunglasses in class?Because her students were so bright!\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert joke writer.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "son_prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
    ")\n",
    "\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "json_chain = json_prompt | llmmodel | json_parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eff8eb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': \"Why did the teacher bring a ladder to school? Because she wanted to 'elevate' the learning experience!\"}\n"
     ]
    }
   ],
   "source": [
    "response = json_chain.invoke({\"question\": \"create a joke for teachers\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb90384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
