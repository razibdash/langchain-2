{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74ca373",
   "metadata": {},
   "source": [
    "chat completion model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5687617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Langchain 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41669ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "from langchain_groq import ChatGroq\n",
    "GROQ_API_KEY=os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c216e3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\langchain_groq\\chat_models.py:370: UserWarning: WARNING! max_completion_tokens is not default parameter.\n",
      "                    max_completion_tokens was transferred to model_kwargs.\n",
      "                    Please confirm that max_completion_tokens is what you intended.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\llmapp\\Lib\\site-packages\\langchain_groq\\chat_models.py:370: UserWarning: WARNING! top_p is not default parameter.\n",
      "                    top_p was transferred to model_kwargs.\n",
      "                    Please confirm that top_p is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the Groq LLM with the model name and parameters\n",
    "llamaChatModel = ChatGroq(\n",
    "    api_key=GROQ_API_KEY,\n",
    "    model=\"llama3-70b-8192\",\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1aeaa2",
   "metadata": {},
   "source": [
    "Prompts and Prompt Templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2393d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here\\'s a curious story about Nikola Tesla:\\n\\n**The Mysterious Death of Tesla\\'s Pigeon**\\n\\nNikola Tesla, the famous inventor and electrical engineer, had a peculiar relationship with a pigeon that has sparked much curiosity and speculation over the years. In the 1920s, Tesla, then in his 60s, developed an unusual bond with a white pigeon that would visit him daily at his hotel room in New York City.\\n\\nThe story goes that Tesla would feed the pigeon and even claimed that they shared a deep emotional connection. He believed that the pigeon was a kindred spirit, a being of superior intelligence that understood him in a way that no human did. Tesla often referred to the pigeon as his \"love\" and \"dearest friend.\"\\n\\nTragically, the pigeon vanished one day, and Tesla was devastated. He searched the city frantically, but to no avail. In his grief, Tesla claimed that he had received a telepathic message from the pigeon, informing him of its impending death. According to Tesla, the pigeon told him that it was going to a \"higher level of existence\" and that their bond would transcend physical death.\\n\\nTesla was so distraught that he stopped working on his projects and became reclusive. His friends and colleagues grew concerned about his mental health, and some even speculated that he had suffered a nervous breakdown.\\n\\nNow, here\\'s the curious part: After the pigeon\\'s disappearance, Tesla began to experience strange occurrences in his laboratory. Electrical devices would malfunction or behave erratically, and Tesla believed that the pigeon\\'s spirit was still communicating with him, guiding his work. He claimed that the pigeon was helping him unlock the secrets of the universe, revealing hidden patterns and energies that he had never seen before.\\n\\nWhile some might dismiss this story as the product of Tesla\\'s eccentricity or even senility, it\\'s worth noting that many of Tesla\\'s contemporaries, including his biographer, John J. O\\'Neill, reported that Tesla\\'s work during this period was exceptionally innovative and productive. Some of his most groundbreaking ideas, including his theories on resonant frequency and alternating current, emerged during this time.\\n\\nThe curious case of Tesla\\'s pigeon has sparked numerous theories and interpretations over the years. Was the pigeon a symbol of Tesla\\'s own creative energy and inspiration? Did it represent a higher, spiritual realm that Tesla was attempting to tap into? Or was it simply a manifestation of Tesla\\'s eccentric personality and his tendency to anthropomorphize objects?\\n\\nWhatever the truth may be, the story of Tesla\\'s pigeon remains an enchanting and mysterious chapter in the life of one of humanity\\'s most brilliant minds.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 532, 'prompt_tokens': 17, 'total_tokens': 549, 'completion_time': 2.079697678, 'prompt_time': 0.000247006, 'queue_time': 0.052825864, 'total_time': 2.079944684}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run--e8a48cdc-861c-4fab-9b09-4b0c6431e7f9-0' usage_metadata={'input_tokens': 17, 'output_tokens': 532, 'total_tokens': 549}\n"
     ]
    }
   ],
   "source": [
    "# This is for completions model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} story about {topic}\"\n",
    ")\n",
    "\n",
    "llmModelPrompt = prompt_template.format(\n",
    "    adjective=\"curious\",\n",
    "    topic=\"Tesla\"\n",
    ")\n",
    "\n",
    "res = llamaChatModel.invoke(llmModelPrompt)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
